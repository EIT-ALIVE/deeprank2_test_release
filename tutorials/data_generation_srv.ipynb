{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation for single-residue variants\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "<img style=\"margin-left: 1.5rem\" align=\"right\" src=\"images/data_generation_variants.png\" width=\"400\">\n",
    "\n",
    "This tutorial will demonstrate the use of DeepRank2 for generating single-residue variants (SRVs) graphs and saving them as [HDF5 files](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) files, using [PBD files](<https://en.wikipedia.org/wiki/Protein_Data_Bank_(file_format)>) of protein structures as input.\n",
    "\n",
    "In this data processing phase, a local neighborhood around the mutated residue is selected for each SRV according to a radius threshold that the user can customize. All atoms or residues within the threshold are mapped as the nodes to a graph and the interactions between them are the edges of the graph. Each node and edge can have several distinct (structural or physico-chemical) features, which are generated and added during the processing phase as well. Optionally, the graphs can be mapped to volumetric grids (i.e., 3D image-like representations), together with their features. Finally, the mapped data are saved as HDF5 files, which can be used for training predictive models (for details see [training_ppi.ipynb](https://github.com/DeepRank/deeprank-core/blob/main/tutorials/training_ppi.ipynb) tutorial). In particular, graphs can be used for the training of Graph Neural Networks (GNNs), and grids can be used for the training of Convolutional Neural Networks (CNNs).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data\n",
    "\n",
    "The example data used in this tutorial are available on Zenodo at [this record address](https://zenodo.org/record/8349335). To download the raw data used in this tutorial, please visit the link and download `data_raw.zip`. Unzip it, and save the `data_raw/` folder in the same directory as this notebook. The name and the location of the folder are optional but recommended, as they are the name and the location we will use to refer to the folder throughout the tutorial.\n",
    "\n",
    "Note that the dataset contains only 96 data points, which is not enough to develop an impactful predictive model, and the scope of its use is indeed only demonstrative and informative for the users.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The libraries needed for this tutorial:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import h5py\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from deeprank2.dataset import GraphDataset\n",
    "from deeprank2.domain.aminoacidlist import amino_acids_by_code\n",
    "from deeprank2.features import components, contact\n",
    "from deeprank2.query import QueryCollection, SingleResidueVariantQuery\n",
    "from deeprank2.utils.grid import GridSettings, MapMethod"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw files and paths\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paths for reading raw data and saving the processed ones:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(\"data_raw\", \"srv\")\n",
    "processed_data_path = os.path.join(\"data_processed\", \"srv\")\n",
    "os.makedirs(os.path.join(processed_data_path, \"residue\"))\n",
    "os.makedirs(os.path.join(processed_data_path, \"atomic\"))\n",
    "# Flag limit_data as True if you are running on a machine with limited memory (e.g., Docker container)\n",
    "limit_data = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Raw data are PDB files in `data_raw/srv/pdb/`, which contains atomic coordinates of the protein structure containing the variant.\n",
    "- Target data, so in our case pathogenic versus benign labels, are in `data_raw/srv/srv_target_values.csv`.\n",
    "- The final SRV processed data will be saved in `data_processed/srv/` folder, which in turns contains a folder for residue-level data and another one for atomic-level data. More details about such different levels will come a few cells below.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_pdb_files_and_target_data` is an helper function used to retrieve the raw pdb files names, SRVs information and target values in a list from the CSV:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdb_files_and_target_data(data_path: str) -> tuple[list[str], list[int], list[str], list[str], list[float]]:\n",
    "    csv_data = pd.read_csv(os.path.join(data_path, \"srv_target_values.csv\"))\n",
    "    pdb_files = glob.glob(os.path.join(data_path, \"pdb\", \"*.ent\"))\n",
    "    pdb_files.sort()\n",
    "    pdb_file_names = [os.path.basename(pdb_file) for pdb_file in pdb_files]\n",
    "    csv_data_indexed = csv_data.set_index(\"pdb_file\")\n",
    "    csv_data_indexed = csv_data_indexed.loc[pdb_file_names]\n",
    "    res_numbers = csv_data_indexed.res_number.tolist()\n",
    "    res_wildtypes = csv_data_indexed.res_wildtype.tolist()\n",
    "    res_variants = csv_data_indexed.res_variant.tolist()\n",
    "    targets = csv_data_indexed.target.tolist()\n",
    "    pdb_names = csv_data_indexed.index.tolist()\n",
    "    pdb_files = [data_path + \"/pdb/\" + pdb_name for pdb_name in pdb_names]\n",
    "\n",
    "    return pdb_files, res_numbers, res_wildtypes, res_variants, targets\n",
    "\n",
    "\n",
    "pdb_files, res_numbers, res_wildtypes, res_variants, targets = get_pdb_files_and_target_data(data_path)\n",
    "\n",
    "if limit_data:\n",
    "    pdb_files = pdb_files[:15]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `QueryCollection` and `Query` objects\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each SRV, so for each data point, a query can be created and added to the `QueryCollection` object, to be processed later on. Different types of queries exist, based on the molecular resolution needed:\n",
    "\n",
    "A query takes as inputs:\n",
    "\n",
    "- A `.pdb` file, representing the protein structure containing the SRV.\n",
    "- The resolution (`\"residue\"` or `\"atom\"`), i.e. whether each node should represent an amino acid residue or an atom.\n",
    "- The chain id of the SRV.\n",
    "- The residue number of the missense mutation.\n",
    "- The insertion code, used when two residues have the same numbering. The combination of residue numbering and insertion code defines the unique residue.\n",
    "- The wildtype amino acid.\n",
    "- The variant amino acid.\n",
    "- The interaction radius, which determines the threshold distance (in Ångström) for residues/atoms surrounding the mutation that will be included in the graph.\n",
    "- The target values associated with the query. For each query/data point, in the use case demonstrated in this tutorial will add a 0 if the SRV belongs to the benign class, and 1 if it belongs to the pathogenic one.\n",
    "- The max edge distance, which is the maximum distance between two nodes to generate an edge between them.\n",
    "- Optional: The correspondent [Position-Specific Scoring Matrices (PSSMs)](https://en.wikipedia.org/wiki/Position_weight_matrix), per chain identifier, in the form of .pssm files. PSSMs are optional and will not be used in this tutorial.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residue-level SRV: `SingleResidueVariantQuery`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = QueryCollection()\n",
    "\n",
    "influence_radius = 10.0  # radius to select the local neighborhood around the SRV\n",
    "max_edge_length = 4.5  # ??\n",
    "\n",
    "print(f\"Adding {len(pdb_files)} queries to the query collection ...\")\n",
    "for i in range(len(pdb_files)):\n",
    "    queries.add(\n",
    "        SingleResidueVariantQuery(\n",
    "            pdb_path=pdb_files[i],\n",
    "            resolution=\"residue\",\n",
    "            chain_ids=\"A\",\n",
    "            variant_residue_number=res_numbers[i],\n",
    "            insertion_code=None,\n",
    "            wildtype_amino_acid=amino_acids_by_code[res_wildtypes[i]],\n",
    "            variant_amino_acid=amino_acids_by_code[res_variants[i]],\n",
    "            targets={\"binary\": targets[i]},\n",
    "            influence_radius=influence_radius,\n",
    "            max_edge_length=max_edge_length,\n",
    "        ),\n",
    "    )\n",
    "    if i + 1 % 20 == 0:\n",
    "        print(f\"{i+1} queries added to the collection.\")\n",
    "\n",
    "print(f\"{i+1} queries ready to be processed.\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on `process()` method\n",
    "\n",
    "Once all queries have been added to the `QueryCollection` instance, they can be processed. Main parameters of the `process()` method, include:\n",
    "\n",
    "- `prefix` sets the output file location.\n",
    "- `feature_modules` allows you to choose which feature generating modules you want to use. By default, the basic features contained in `deeprank2.features.components` and `deeprank2.features.contact` are generated. Users can add custom features by creating a new module and placing it in the `deeprank2.feature` subpackage. A complete and detailed list of the pre-implemented features per module and more information about how to add custom features can be found [here](https://deeprank2.readthedocs.io/en/latest/features.html).\n",
    "  - Note that all features generated by a module will be added if that module was selected, and there is no way to only generate specific features from that module. However, during the training phase shown in `training_ppi.ipynb`, it is possible to select only a subset of available features.\n",
    "- `cpu_count` can be used to specify how many processes to be run simultaneously, and will coincide with the number of HDF5 files generated. By default it takes all available CPU cores and HDF5 files are squashed into a single file using the `combine_output` setting.\n",
    "- Optional: If you want to include grids in the HDF5 files, which represent the mapping of the graphs to a volumetric box, you need to define `grid_settings` and `grid_map_method`, as shown in the example below. If they are `None` (default), only graphs are saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_settings = GridSettings(  # None if you don't want grids\n",
    "    # the number of points on the x, y, z edges of the cube\n",
    "    points_counts=[35, 30, 30],\n",
    "    # x, y, z sizes of the box in Å\n",
    "    sizes=[1.0, 1.0, 1.0],\n",
    ")\n",
    "grid_map_method = MapMethod.GAUSSIAN  # None if you don't want grids\n",
    "\n",
    "queries.process(\n",
    "    prefix=os.path.join(processed_data_path, \"residue\", \"proc\"),\n",
    "    feature_modules=[components, contact],\n",
    "    cpu_count=8,\n",
    "    combine_output=False,\n",
    "    grid_settings=grid_settings,\n",
    "    grid_map_method=grid_map_method,\n",
    ")\n",
    "\n",
    "print(f'The queries processing is done. The generated HDF5 files are in {os.path.join(processed_data_path, \"residue\")}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring data\n",
    "\n",
    "As representative example, the following is the HDF5 structure generated by the previous code for `pdb2ooh.ent`, so for one single graph, which represents one protein structure containing a SRV in position 112, for the graph + grid case:\n",
    "\n",
    "```bash\n",
    "└── residue-graph:A:112:Threonine->Isoleucine:pdb2ooh\n",
    "    |\n",
    "    ├── edge_features\n",
    "    │   ├── _index\n",
    "    │   ├── _name\n",
    "    │   ├── covalent\n",
    "    │   ├── distance\n",
    "    │   ├── electrostatic\n",
    "    │   ├── same_chain\n",
    "    │   └── vanderwaals\n",
    "    |\n",
    "    ├── node_features\n",
    "    │   ├── _chain_id\n",
    "    │   ├── _name\n",
    "    │   ├── _position\n",
    "    │   ├── diff_charge\n",
    "    │   ├── diff_hb_donors\n",
    "    │   ├── diff_hb_acceptors\n",
    "    │   ├── diff_mass\n",
    "    │   ├── diff_pI\n",
    "    │   ├── diff_polarity\n",
    "    │   ├── diff_size\n",
    "    │   ├── hb_acceptors\n",
    "    │   ├── hb_donors\n",
    "    │   ├── polarity\n",
    "    │   ├── res_charge\n",
    "    │   ├── res_mass\n",
    "    |   ├── res_pI\n",
    "    |   ├── res_size\n",
    "    |   ├── res_type\n",
    "    |   └── variant_res\n",
    "    |\n",
    "    ├── grid_points\n",
    "    │   ├── center\n",
    "    │   ├── x\n",
    "    │   ├── y\n",
    "    │   └── z\n",
    "    |\n",
    "    ├── mapped_features\n",
    "    │   ├── _position_000\n",
    "    │   ├── _position_001\n",
    "    │   ├── _position_002\n",
    "    │   ├── covalent\n",
    "    │   ├── distance\n",
    "    │   ├── electrostatic\n",
    "    │   ├── diff_polarity_000\n",
    "    │   ├── diff_polarity_001\n",
    "    │   ├── diff_polarity_002\n",
    "    │   ├── diff_polarity_003\n",
    "    |   ├── ...\n",
    "    |   └── vanderwaals\n",
    "    |\n",
    "    └── target_values\n",
    "        └── binary\n",
    "```\n",
    "\n",
    "`edge_features`, `node_features`, `mapped_features` are [HDF5 Groups](https://docs.h5py.org/en/stable/high/group.html) which contain [HDF5 Datasets](https://docs.h5py.org/en/stable/high/dataset.html) (e.g., `_index`, `electrostatic`, etc.), which in turn contains features values in the form of arrays. `edge_features` and `node_features` refer specificly to the graph representation, while `grid_points` and `mapped_features` refer to the grid mapped from the graph. Each data point generated by deeprank2 has the above structure, with the features and the target changing according to the user's settings. Features starting with `_` are present for human inspection of the data, but they are not used for training models.\n",
    "\n",
    "It is always a good practice to first explore the data, and then make decision about splitting them in training, test and validation sets. There are different possible ways for doing it.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas dataframe\n",
    "\n",
    "The edge and node features just generated can be explored by instantiating the `GraphDataset` object, and then using `hdf5_to_pandas` method which converts node and edge features into a [Pandas](https://pandas.pydata.org/) dataframe. Each row represents a ppi in the form of a graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = glob.glob(os.path.join(processed_data_path, \"residue\", \"*.hdf5\"))\n",
    "dataset = GraphDataset(processed_data, target=\"binary\")\n",
    "dataset_df = dataset.hdf5_to_pandas()\n",
    "dataset_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate histograms for looking at the features distributions. An example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join(processed_data_path, \"residue\", \"res_mass_distance_electrostatic\")\n",
    "\n",
    "dataset.save_hist(features=[\"res_mass\", \"distance\", \"electrostatic\"], fname=fname)\n",
    "\n",
    "im = img.imread(fname + \".png\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "fig = plt.imshow(im)\n",
    "fig.axes.get_xaxis().set_visible(False)\n",
    "fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other tools\n",
    "\n",
    "- [HDFView](https://www.hdfgroup.org/downloads/hdfview/), a visual tool written in Java for browsing and editing HDF5 files.\n",
    "  As representative example, the following is the structure for `pdb2ooh.ent` seen from HDF5View:\n",
    "\n",
    "  <img style=\"margin-bottom: 1.5rem\" align=\"centrum\" src=\"images/hdfview_variant.png\" width=\"200\">\n",
    "\n",
    "  Using this tool you can inspect the values of the features visually, for each data point.\n",
    "\n",
    "- Python packages such as [h5py](https://docs.h5py.org/en/stable/index.html). Examples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(processed_data[0], \"r\") as hdf5:\n",
    "    # List of all graphs in hdf5, each graph representing\n",
    "    # a SRV and its sourrouding environment\n",
    "    ids = list(hdf5.keys())\n",
    "    print(f\"IDs of SRVs in {processed_data[0]}: {ids}\")\n",
    "    node_features = list(hdf5[ids[0]][\"node_features\"])\n",
    "    print(f\"Node features: {node_features}\")\n",
    "    edge_features = list(hdf5[ids[0]][\"edge_features\"])\n",
    "    print(f\"Edge features: {edge_features}\")\n",
    "    target_features = list(hdf5[ids[0]][\"target_values\"])\n",
    "    print(f\"Targets features: {target_features}\")\n",
    "    # Polarity feature for ids[0], numpy.ndarray\n",
    "    node_feat_polarity = hdf5[ids[0]][\"node_features\"][\"polarity\"][:]\n",
    "    print(f\"Polarity feature shape: {node_feat_polarity.shape}\")\n",
    "    # Electrostatic feature for ids[0], numpy.ndarray\n",
    "    edge_feat_electrostatic = hdf5[ids[0]][\"edge_features\"][\"electrostatic\"][:]\n",
    "    print(f\"Electrostatic feature shape: {edge_feat_electrostatic.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atomic-level SRV: `SingleResidueVariantQuery`\n",
    "\n",
    "Graphs can also be generated at an atomic resolution, very similarly to what has just been done for residue-level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = QueryCollection()\n",
    "\n",
    "influence_radius = 10.0  # radius to select the local neighborhood around the SRV\n",
    "max_edge_length = 4.5  # ??\n",
    "\n",
    "print(f\"Adding {len(pdb_files)} queries to the query collection ...\")\n",
    "for i in range(len(pdb_files)):\n",
    "    queries.add(\n",
    "        SingleResidueVariantQuery(\n",
    "            pdb_path=pdb_files[i],\n",
    "            resolution=\"atom\",\n",
    "            chain_ids=\"A\",\n",
    "            variant_residue_number=res_numbers[i],\n",
    "            insertion_code=None,\n",
    "            wildtype_amino_acid=amino_acids_by_code[res_wildtypes[i]],\n",
    "            variant_amino_acid=amino_acids_by_code[res_variants[i]],\n",
    "            targets={\"binary\": targets[i]},\n",
    "            influence_radius=influence_radius,\n",
    "            max_edge_length=max_edge_length,\n",
    "        ),\n",
    "    )\n",
    "    if i + 1 % 20 == 0:\n",
    "        print(f\"{i+1} queries added to the collection.\")\n",
    "\n",
    "print(f\"{i+1} queries ready to be processed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_settings = GridSettings(  # None if you don't want grids\n",
    "    # the number of points on the x, y, z edges of the cube\n",
    "    points_counts=[35, 30, 30],\n",
    "    # x, y, z sizes of the box in Å\n",
    "    sizes=[1.0, 1.0, 1.0],\n",
    ")\n",
    "grid_map_method = MapMethod.GAUSSIAN  # None if you don't want grids\n",
    "\n",
    "queries.process(\n",
    "    prefix=os.path.join(processed_data_path, \"atomic\", \"proc\"),\n",
    "    feature_modules=[components, contact],\n",
    "    cpu_count=8,\n",
    "    combine_output=False,\n",
    "    grid_settings=grid_settings,\n",
    "    grid_map_method=grid_map_method,\n",
    ")\n",
    "\n",
    "print(f'The queries processing is done. The generated HDF5 files are in {os.path.join(processed_data_path, \"atomic\")}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the data can be inspected using `hdf5_to_pandas` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = glob.glob(os.path.join(processed_data_path, \"atomic\", \"*.hdf5\"))\n",
    "dataset = GraphDataset(processed_data, target=\"binary\")\n",
    "dataset_df = dataset.hdf5_to_pandas()\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join(processed_data_path, \"atomic\", \"atom_charge\")\n",
    "dataset.save_hist(features=\"atom_charge\", fname=fname)\n",
    "\n",
    "im = img.imread(fname + \".png\")\n",
    "plt.figure(figsize=(8, 8))\n",
    "fig = plt.imshow(im)\n",
    "fig.axes.get_xaxis().set_visible(False)\n",
    "fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some of the features are different from the ones generated with the residue-level queries. There are indeed features in `deeprank2.features.components` module which are generated only in atomic graphs, i.e. `atom_type`, `atom_charge`, and `pdb_occupancy`, because they don't make sense only in the atomic graphs' representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprankcore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
