{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pkg_resources as pkg\n",
    "\n",
    "PATH_DEEPRANK_CORE = Path(pkg.resource_filename(\"deeprank2\", \"\"))\n",
    "ROOT = PATH_DEEPRANK_CORE.parent\n",
    "PATH_TEST = ROOT / \"tests\"\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "from deeprank2.dataset import save_hdf5_keys\n",
    "from deeprank2.domain.aminoacidlist import alanine, phenylalanine\n",
    "from deeprank2.query import (\n",
    "    ProteinProteinInterfaceQuery,\n",
    "    QueryCollection,\n",
    "    SingleResidueVariantQuery,\n",
    ")\n",
    "from deeprank2.tools.target import compute_ppi_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generating 1ATN_ppi.hdf5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from Bio import BiopythonWarning\n",
    "\n",
    "from deeprank2.utils.grid import GridSettings, MapMethod\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", BiopythonWarning)\n",
    "    warnings.simplefilter(\"ignore\", RuntimeWarning)\n",
    "\n",
    "    ref_path = str(PATH_TEST / \"data/ref/1ATN/1ATN.pdb\")\n",
    "    pssm_path1 = str(PATH_TEST / \"data/pssm/1ATN/1ATN.A.pdb.pssm\")\n",
    "    pssm_path2 = str(PATH_TEST / \"data/pssm/1ATN/1ATN.B.pdb.pssm\")\n",
    "    chain_id1 = \"A\"\n",
    "    chain_id2 = \"B\"\n",
    "    pdb_paths = [\n",
    "        str(PATH_TEST / \"data/pdb/1ATN/1ATN_1w.pdb\"),\n",
    "        str(PATH_TEST / \"data/pdb/1ATN/1ATN_2w.pdb\"),\n",
    "        str(PATH_TEST / \"data/pdb/1ATN/1ATN_3w.pdb\"),\n",
    "        str(PATH_TEST / \"data/pdb/1ATN/1ATN_4w.pdb\"),\n",
    "    ]\n",
    "\n",
    "    queries = QueryCollection()\n",
    "\n",
    "    for pdb_path in pdb_paths:\n",
    "        # Append data points\n",
    "        targets = compute_ppi_scores(pdb_path, ref_path)\n",
    "        queries.add(\n",
    "            ProteinProteinInterfaceQuery(\n",
    "                pdb_path=pdb_path,\n",
    "                resolution=\"residue\",\n",
    "                chain_ids=[chain_id1, chain_id2],\n",
    "                targets=targets,\n",
    "                pssm_paths={chain_id1: pssm_path1, chain_id2: pssm_path2},\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Generate graphs and save them in hdf5 files\n",
    "    output_paths = queries.process(\n",
    "        cpu_count=1,\n",
    "        prefix=\"1ATN_ppi\",\n",
    "        grid_settings=GridSettings([20, 20, 20], [20.0, 20.0, 20.0]),\n",
    "        grid_map_method=MapMethod.GAUSSIAN,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generating residue.hdf5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local data\n",
    "project_folder = \"/home/dbodor/git/DeepRank/deeprank-core/tests/data/sample_25_07122022/\"\n",
    "csv_file_name = \"BA_pMHCI_human_quantitative.csv\"\n",
    "models_folder_name = \"exp_nmers_all_HLA_quantitative\"\n",
    "data = \"pMHCI\"\n",
    "resolution = \"residue\"  # either 'residue' or 'atom'\n",
    "influence_radius = 15  # max distance in Å between two interacting residues/atoms of two proteins\n",
    "max_edge_length = 15  # max distance in Å between to create an edge\n",
    "\n",
    "csv_file_path = f\"{project_folder}data/external/processed/I/{csv_file_name}\"\n",
    "models_folder_path = f\"{project_folder}data/{data}/features_input_folder/{models_folder_name}\"\n",
    "\n",
    "pdb_files = glob.glob(os.path.join(models_folder_path + \"/pdb\", \"*.pdb\"))\n",
    "pdb_files.sort()\n",
    "print(f\"{len(pdb_files)} pdbs found.\")\n",
    "pssm_m = glob.glob(os.path.join(models_folder_path + \"/pssm\", \"*.M.*.pssm\"))\n",
    "pssm_m.sort()\n",
    "print(f\"{len(pssm_m)} MHC pssms found.\")\n",
    "pssm_p = glob.glob(os.path.join(models_folder_path + \"/pssm\", \"*.P.*.pssm\"))\n",
    "pssm_p.sort()\n",
    "print(f\"{len(pssm_p)} peptide pssms found.\")\n",
    "csv_data = pd.read_csv(csv_file_path)\n",
    "csv_data.cluster = csv_data.cluster.fillna(-1)\n",
    "pdb_ids_csv = [pdb_file.split(\"/\")[-1].split(\".\")[0].replace(\"-\", \"_\") for pdb_file in pdb_files]\n",
    "clusters = [csv_data[pdb_id == csv_data.ID].cluster.to_numpy()[0] for pdb_id in pdb_ids_csv]\n",
    "bas = [csv_data[pdb_id == csv_data.ID].measurement_value.to_numpy()[0] for pdb_id in pdb_ids_csv]\n",
    "\n",
    "queries = QueryCollection()\n",
    "print(f\"Adding {len(pdb_files)} queries to the query collection ...\")\n",
    "for i in range(len(pdb_files)):\n",
    "    queries.add(\n",
    "        ProteinProteinInterfaceQuery(\n",
    "            pdb_path=pdb_files[i],\n",
    "            resolution=\"residue\",\n",
    "            chain_ids=[\"M\", \"P\"],\n",
    "            influence_radius=influence_radius,\n",
    "            max_edge_length=max_edge_length,\n",
    "            targets={\n",
    "                \"binary\": int(float(bas[i]) <= 500),  # binary target value\n",
    "                \"BA\": bas[i],  # continuous target value\n",
    "                \"cluster\": clusters[i],\n",
    "            },\n",
    "            pssm_paths={\"M\": pssm_m[i], \"P\": pssm_p[i]},\n",
    "        ),\n",
    "    )\n",
    "print(\"Queries created and ready to be processed.\\n\")\n",
    "\n",
    "output_paths = queries.process(prefix=\"residue\")\n",
    "print(output_paths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generating train.hdf5, valid.hdf5, test.hdf5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing hdf5 file in train, valid, test\n",
    "hdf5_path = \"residue.hdf5\"\n",
    "train_clusters = [3, 4, 5, 2]\n",
    "val_clusters = [1, 8]\n",
    "test_clusters = [6]\n",
    "target = \"target_values\"\n",
    "feature = \"cluster\"\n",
    "\n",
    "clusters = {}\n",
    "train_ids = []\n",
    "val_ids = []\n",
    "test_ids = []\n",
    "\n",
    "with h5py.File(hdf5_path, \"r\") as hdf5:\n",
    "    for key in hdf5:\n",
    "        feature_value = float(hdf5[key][target][feature][()])\n",
    "        if feature_value in train_clusters:\n",
    "            train_ids.append(key)\n",
    "        elif feature_value in val_clusters:\n",
    "            val_ids.append(key)\n",
    "        elif feature_value in test_clusters:\n",
    "            test_ids.append(key)\n",
    "\n",
    "        if feature_value in clusters:\n",
    "            clusters[int(feature_value)] += 1\n",
    "        else:\n",
    "            clusters[int(feature_value)] = 1\n",
    "\n",
    "    print(f\"Trainset contains {len(train_ids)} data points, {round(100*len(train_ids)/len(hdf5.keys()), 2)}% of the total data.\")\n",
    "    print(f\"Validation set contains {len(val_ids)} data points, {round(100*len(val_ids)/len(hdf5.keys()), 2)}% of the total data.\")\n",
    "    print(f\"Test set contains {len(test_ids)} data points, {round(100*len(test_ids)/len(hdf5.keys()), 2)}% of the total data.\\n\")\n",
    "\n",
    "    for key, value in dict(sorted(clusters.items(), key=lambda x: x[1], reverse=True)).items():\n",
    "        print(f\"Group with value {key}: {value} data points, {round(100*value/len(hdf5.keys()), 2)}% of total data.\")\n",
    "\n",
    "save_hdf5_keys(hdf5_path, train_ids, \"train.hdf5\", hardcopy=True)\n",
    "save_hdf5_keys(hdf5_path, val_ids, \"valid.hdf5\", hardcopy=True)\n",
    "save_hdf5_keys(hdf5_path, test_ids, \"test.hdf5\", hardcopy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generating variants.hdf5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_queries = 5\n",
    "pdb_path = str(PATH_TEST / \"data/pdb/3C8P/3C8P.pdb\")\n",
    "ref_path = str(PATH_TEST / \"data/ref/3C8P/3C8P.pdb\")\n",
    "targets = compute_ppi_scores(pdb_path, ref_path)\n",
    "queries = QueryCollection()\n",
    "\n",
    "for number in range(1, count_queries + 1):\n",
    "    query = SingleResidueVariantQuery(\n",
    "        pdb_path=pdb_path,\n",
    "        resolution=\"residue\",\n",
    "        chain_ids=\"A\",\n",
    "        variant_residue_number=number,\n",
    "        insertion_code=None,\n",
    "        wildtype_amino_acid=alanine,\n",
    "        variant_amino_acid=phenylalanine,\n",
    "        pssm_paths={\"A\": str(PATH_TEST / \"data/pssm/3C8P/3C8P.A.pdb.pssm\"), \"B\": str(PATH_TEST / \"data/pssm/3C8P/3C8P.B.pdb.pssm\")},\n",
    "        targets=targets,\n",
    "    )\n",
    "    queries.add(query)\n",
    "\n",
    "output_paths = queries.process(cpu_count=1, prefix=\"variants\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generating atom.hdf5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_path = str(PATH_TEST / \"data/ref/1ATN/1ATN.pdb\")\n",
    "pssm_path1 = str(PATH_TEST / \"data/pssm/1ATN/1ATN.A.pdb.pssm\")\n",
    "pssm_path2 = str(PATH_TEST / \"data/pssm/1ATN/1ATN.B.pdb.pssm\")\n",
    "chain_id1 = \"A\"\n",
    "chain_id2 = \"B\"\n",
    "pdb_paths = [\n",
    "    str(PATH_TEST / \"data/pdb/1ATN/1ATN_1w.pdb\"),\n",
    "    str(PATH_TEST / \"data/pdb/1ATN/1ATN_2w.pdb\"),\n",
    "    str(PATH_TEST / \"data/pdb/1ATN/1ATN_3w.pdb\"),\n",
    "    str(PATH_TEST / \"data/pdb/1ATN/1ATN_4w.pdb\"),\n",
    "]\n",
    "\n",
    "queries = QueryCollection()\n",
    "\n",
    "for pdb_path in pdb_paths:\n",
    "    # Append data points\n",
    "    targets = compute_ppi_scores(pdb_path, ref_path)\n",
    "    queries.add(\n",
    "        ProteinProteinInterfaceQuery(\n",
    "            pdb_path=pdb_path,\n",
    "            resolution=\"atom\",\n",
    "            chain_ids=[chain_id1, chain_id2],\n",
    "            targets=targets,\n",
    "            pssm_paths={chain_id1: pssm_path1, chain_id2: pssm_path2},\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# Generate graphs and save them in hdf5 files\n",
    "output_paths = queries.process(cpu_count=1, prefix=\"atom\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('deeprank')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "276ef95c56205118bc396c7ecff9f4bbaf55abeabfc798eaf375738c8dd159eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
